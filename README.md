# Task 5 â€“ Decision Trees & Random Forests

## ðŸ“Œ Overview

This repository contains my solution for **AI/ML Internship Task 5**. The goal is to learn and apply **Decision Trees** and **Random Forests** for classification, understand overfitting, interpret feature importance, and evaluate using cross-validation.

## ðŸ“‚ Contents

* `Task5_DecisionTrees_RandomForest.ipynb` â€“ Jupyter notebook with full code & visualizations
* `README.md` â€“ project details

## ðŸ§® Steps Performed

1. Trained a **Decision Tree Classifier** and visualized it
2. Analyzed **overfitting** and controlled depth (max_depth tuning)
3. Trained a **Random Forest Classifier** and compared accuracy with Decision Tree
4. Interpreted **feature importances**
5. Evaluated models using **cross-validation**

## ðŸ“Š Key Results

* **Unpruned Decision Tree** â†’ high training accuracy but overfit on test set
* **Pruned Decision Tree** â†’ better generalization
* **Random Forest** â†’ higher accuracy and more stable results than a single tree
* **Feature Importances** highlight the most relevant predictors

## ðŸš€ How to Run

1. Upload dataset (e.g., `heart.csv`) or use default sklearn dataset.
2. Open and run the notebook in **Google Colab** or Jupyter.
3. View results, visualizations, and metrics directly in the notebook.

